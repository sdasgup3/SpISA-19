\section{Instruction Decoding}

While a semantics of \GENISA is valuable on its own, its direct uses are limited alone.  Very few real world programs are most conveniently available in assembler form, and not source or binary forms.   Formalizations for many source code languages already exist, and thus would be a natural choice in the event that source code is available.  However, many programs must make use of precompiled binaries for which source is not available – either due to IP concerns, the circumstances of the researcher (as an example, malware researchers) or due to the age of the program.  In order to reason about these programs, one must work from what one has: an executable binary file.  Thus, in order to apply the existing semantics of \GENISA to more common real world programs, the semantics must incorporate a disassembler of some form to translate from binary back to the modeled assembly language.

Two basic approaches to applying binaries to the existing x86 semantics present themselves.  First, one could disassemble the entire binary into a set of x86 assembler files, and then pass those directly to run on the semantics unmodified.  Alternatively, one could formalize the semantics of instruction decoding, and perform the decoding at runtime with the semantics.  The first option would seem relatively simpler, as it would allow the use of existing tools to perform the decoding.  However, disassembly of x86 programs is no trivial task.  While there exist many tools capable of disassembling a binary, they are forced to make certain assumptions about the programs under disassembly.  The basic algorithms are as follows:

\begin{enumerate}
    \item Linear sweep: Starting from the beginning of the text segment, disassemble instructions sequentially over the entire program.  This approach is the simplest, but breaks if it starts at an incorrect offset, or unexpected data appears in the text segment of the program.
    \item Recursive descent: Starting at a known address (e.g. the program entry), decode sequentially until a branch is detected, in which case spawn another thread which decodes the target.
    \item Probabilistic: ~\cite{Miller:2019}
\end{enumerate}

However, each of these algorithms share a common flaw: they cannot offer any certainty that the decoded program is actually the one that would run if the binary were executed, and thus conclusions drawn from that program must be met with some skepticism.  As an example of why, consider the target of an indirect jump (of the form \instr{jmp \%rax}) – where the target is computed by an arbitrary function.  Determining the value of that function’s return can be arbitrarily complex, up to undecidable.  Thus, the location of program flow is, in the general case, undecidable.  Few, if any, real world x86 programs lack any indirect jumps, as \instr{ret} performs one. 

The alternative approach of decoding individual instructions as the semantics execute them sidesteps this issue.  At any given step of execution, the semantics know exactly where the program counter is, and thus exactly where the next instruction should start.  After decoding and executing the instruction, the semantics know exactly where the program counter should go next (from the size and semantics of the instruction decoded).  This however requires a formalization of instruction decoding in \K, in order to execute the decoder in tandem with the semantics.  To avoid reinventing the wheel, we chose to base the formalized decoder implementation on an existing implementation - Intel’s x86 instruction decoding and disassembly tool, XED~\cite{xed}.  We modified the algorithm to better fit the application and environment (notably, the \K programming language). 

An \GENISA instruction can be broken down into $0-4$ prefixes, the opcode byte(s), the MODRM byte, the SIB byte, the displacement and the immediate(s).  Of these sections, all but the opcodes are optional.  One cannot know how many of these sections exist, or how to interpret the values contained in them until they have decoded previous sections.  So the first step of the algorithm is to look for prefixes, especially \s{VEX} and \s{EVEX} prefixes, each of which will cause future opcode values to be interpreted differently.  The presence of other prefixes is also recorded, and using this information the opcode is found.  Using the information about opcode and prefixes, the presence or absence of the MODRM and SIB bytes are determined, and these bytes are picked apart for their constituent values.  After all this data has been extracted, the precise instruction and operand variant can be determined.  This is where the \K Framework in particular shines – matching arbitrary subsets of properties to particular patterns is the foundation of both the \K rule, and this pivotal instruction selection step.  This match gives this step an extremely intuitive representation in \K – one rule per instruction variant.  \revisit{Moreover, the input data fed to the automatically generated sections of the original Intel decoder can be re-purposed to automatically generate these \K rules<<This is not clear. Did  you mean to mention how you modeled the XED decoder using K. If yes, then can you explain that in bit more detail.>>}.  After instruction selection, the sizes and positions of the remaining sections of the instructions are known to the decoder, and it can extract them trivially.  After this point, the next challenge is using the decoded instruction to generate the desired semantics operation.  Unfortunately, differences in standardizations between the AT\&T syntax implemented by GAS~\cite{gas} (which the most complete \ISA semantics, modeled in \K, were based on) and the AT\&T syntax implemented by XED, along with changes in instruction mnemonics between input assembler code and output disassembled code makes this operation nontrivial.  This problem was eventually solved by building a lookup table of assembled instruction variants to original assembler mnemonics using GAS.  Once this step was completed, translating the instruction operands was relatively simple.

The decoder was tested independently of the semantics (by running in linear sweep mode over a binary and comparing to the reference implementation), as well as running a large number of the gcc torture tests under the semantics. /* Evaluation Stub Section*/ 

The new decode $+$ execution semantics is much more practically useful than the original semantics, but still has some limitations.  Dynamic linking is not supported, so the binary must be statically linked (or the dynamic sections made unreachable).  Presently the semantics do not support system calls so, although the decoder can decode them, they cannot be executed properly.  This leads to a need for some debugging symbols, as the semantics must skip to the \emph{<main>} symbol rather than starting at the program entry point, since most libc implementations’ initialization code will always make system calls.  Future work will be addressing these shortcomings, and addressing technical incompatibilities that currently block symbolic execution over the semantics.

