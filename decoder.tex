\section{Instruction Decoding}

While a semantics of \GENISA mnemonics is valuable on its own, its direct uses are limited alone.  Very few real world programs are most conveniently available in assembler form, and not source or binary forms.   Formalizations for many source code languages already exist, and thus would be a natural choice in the event that source code is available.  However, many programs must make use of precompiled binaries for which source is not available – either due to IP concerns, the circumstances of the researcher (as an example, malware researchers) or due to the age of the program.  In order to reason about these programs, one must work from what one has: an executable binary file.  Thus, in order to apply the existing semantics of \GENISA to more common real world programs, the semantics must incorporate a disassembler of some form to translate from binary back to the modeled assembly language.
\subsection{Approach}
Two basic approaches to applying binaries to the existing x86 semantics present themselves.  First, one could disassemble the entire binary into a set of x86 assembler files, and then pass those directly to run on the semantics unmodified.  Alternatively, one could formalize the semantics of instruction decoding, and perform the decoding at runtime with the semantics.  The first option would seem relatively simpler, as it would allow the use of existing tools to perform the decoding.  However, disassembly of x86 programs is no trivial task.  While there exist many tools capable of disassembling a binary, they are forced to make certain assumptions about the programs under disassembly.  The basic algorithms are as follows:

\begin{enumerate}
    \item Linear sweep: Starting from the beginning of the text segment, disassemble instructions sequentially over the entire program.  This approach is the simplest, but breaks if it starts at an incorrect offset, or unexpected data appears in the text segment of the program.
    \item Recursive descent: Starting at a known address (e.g. the program entry), decode sequentially until a branch is detected, in which case spawn another thread which decodes the target.
    \item Probabilistic~\cite{Miller:2019}: Use features of real chains of instructions (such as a register write followed by a read to the same register) to determine whether a given byte is likely part of an instruction and, if so, at what offset that instruction is most likely to begin.
\end{enumerate}

However, each of these algorithms share a common flaw: they cannot offer any formal certainty that the decoded program is actually the one that would run if the binary were executed, and thus any property proved from that program must be met with some skepticism.  As an example of why, consider the target of an indirect jump (of the form \instr{jmp \%rax}) – where the target is computed by an arbitrary function.  Determining the value of that function’s return can be arbitrarily complex, up to undecidable.  Thus, the location of program flow is, in the general case, undecidable.  Even if no deliberate example of an indirect jump to a computed location exists, one can still occur through a programming error such as a buffer overflow.  Few, if any, real world x86 programs lack any indirect jumps, as \instr{ret} performs one.  Thus, any attempt to prove a property over a disassembled binary must solve this problem.  Hence, the need for a formalized decoder.

In order to sidestep these issues, we took the alternative approach of decoding individual instructions as the semantics execute them.  At any given step of execution, the semantics know exactly where the program counter is, and thus exactly where the next instruction should start.  After decoding and executing the instruction, the semantics know exactly where the program counter should go next (from the size and semantics of the instruction decoded).  This however requires a formalization of instruction decoding in \K, in order to execute the decoder in tandem with the semantics.  To avoid reinventing the wheel, we chose to base the formalized decoder implementation on an existing implementation - Intel’s x86 instruction decoding and disassembly tool, XED~\cite{xed}.  We ported their decoding algorithm to \K, and modified it to better fit the application and environment. 

An \GENISA instruction can be broken down into $0-4$ prefixes, the opcode byte(s), the MODRM byte, the SIB byte, the displacement and the immediate(s).  Of these sections, all but the opcodes are optional.  One cannot know how many of these sections exist, or how to interpret the values contained in them until they have decoded previous sections.  So the first step of the algorithm is to look for prefixes, especially \s{VEX} and \s{EVEX} prefixes, each of which will cause future opcode values to be interpreted differently.  The presence of other prefixes is also recorded, and using this information the opcode is found.  Using the information about opcode and prefixes, the presence or absence of the MODRM and SIB bytes are determined, and these bytes are picked apart for their constituent values.  After all this data has been extracted, the precise instruction and operand variant can be determined.  This is where the \K Framework in particular shines – matching arbitrary subsets of properties to particular patterns is the foundation of both the \K rule, and this pivotal instruction selection step.  This match gives this step an extremely intuitive representation in \K – one rule per instruction variant.  Moreover, the input data fed to the automatically generated sections of the original Intel decoder can be re-purposed to automatically generate these \K rules.  The reference decoder requires complex generated code to perform what amounts to pattern matching on various decode properties that can be known without the instruction variant (as an example, branching between two possible sets of variants on whether the MOD bits of MODRM were 0b11).  \K can do all of these pattern matches in one step, and in a far more legible format by simply incorporating the desired constraint into a rule. After instruction selection, the sizes and positions of the remaining sections of the instructions are known to the decoder, and it can extract them trivially.  After this point, the next challenge is using the decoded instruction to generate the desired semantics operation.  Unfortunately AT\&T syntax (which the most complete \ISA semantics~\cite{Dasgupta:2019}, modeled in \K, were based on) lacks clear standardization over all of x86, and its implementation by GAS~\cite{gas} differs from its implementation by XED (for example, in how it produces suffixes for certain instructions), along with changes in instruction mnemonics between input assembler code and output disassembled code makes mapping decoded instruction mnemonics (decoder output) to assembly instruction mnemonics (semantics input) nontrivial.  This problem was eventually solved by building a lookup table of assembled instruction variants to original assembler mnemonics using GAS.  Once this step was completed, translating the instruction operands was relatively simple.

At first, the decoder was tested on its ability to  convert any binary sequence, specifying to a valid instruction in \ISA, to the corresponding mnemonic. The binary sequences are  obtained either from XED test-suite or created manually using GAS~\cite{gas} assembler. A successful completion of this experiment ensured that the decoding logic is correctly implemented and works for all the valid \ISA instructions. To gain more confidence, the decoder was also tested by combining the instruction disassembler with a simple linear sweep algorithm and then comparing it with the output of XED.  Once an acceptable accuracy was reached, the decoder was combined with the x86 semantics and run on a selection of the gcc-c torture tests~\cite{CTORTURE}.  The tests were modified slightly from their original implementations by replacing certain standard library implementations with simpler versions.  This was done for the sake of feasibility, as the semantics run much more slowly than native code.  As a ground truth, a similarly modifed version of the gcc torture tests were executed normally, and any that did not pass (mostly, due to segmentation faults) running natively due to the simplified stdlib implementations were removed from the selection. This is a limitation of the semantics, due to the lack of a simulated OS for a reasonable implementation, and a need to optimize memory performance, we have omitted a concept of memory page ownership, and thus simulated programs at present will not fault. The gcc torture tests seem generally to be structured such that the test case will either crash or call abort in the event of an error.  This permitted us to avoid needing to compare process memory images to semantics memory images (though such a comparison would doubtlessly be useful - we leave it to future work).  A test was considered to run successfully if the semantics produced an $exit_0$ symbol on completion.  The initial selection contained 498 tests.  Of the 482 of the selected torture tests that executed successfully natively, 468 (97.1\%) executed successfully under the semantics.  Note that the tests were run with a timeout, and many of the failures were the test running out of time (and this number of failures was very slightly nondeterministic due to background tasks on the machine).  The failures were a mix of programs that used unsupported instructions, programs which took too long and timed out, programs that called abort and programs that crashed or aborted during execution.  Though not all passed, the vast majority of the tests did (and, doubtlessly a longer timeout would have allowed several of the failure cases to pass), and continuing work will see this number increase.

\subsection{Limitation}
The new decode $+$ execution semantics is much more practically useful than the original semantics, but still has some limitations.  Dynamic linking is not supported, so the binary must be statically linked (or the dynamic sections made unreachable).  Presently the semantics do not support system calls so, although the decoder can decode them, they cannot be executed properly.  This leads to a need for some debugging symbols, as the semantics must skip to the \emph{<main>} symbol rather than starting at the program entry point, since most libc implementations’ initialization code will always make system calls.  Future work will be addressing these shortcomings, and addressing technical incompatibilities that currently block symbolic execution over the semantics.

